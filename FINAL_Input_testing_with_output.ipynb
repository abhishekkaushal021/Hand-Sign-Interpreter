{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=36992, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=28, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3)\n",
    "\n",
    "        x = torch.randn(IMG_SIZE,IMG_SIZE).view(-1,1,IMG_SIZE,IMG_SIZE)\n",
    "        self._to_linear = None\n",
    "        self.convs(x)\n",
    "\n",
    "        self.fc1 = nn.Linear(self._to_linear, 512) #flattening step \n",
    "        self.fc2 = nn.Linear(512, 28)\n",
    "\n",
    "    def convs(self, x):\n",
    "        drp = nn.Dropout(0.25)\n",
    "        \n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        x = drp(x)\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), (2, 2))\n",
    "        x = drp(x)\n",
    "        x = F.max_pool2d(F.relu(self.conv3(x)), (2, 2))\n",
    "        x = drp(x)\n",
    "\n",
    "        if self._to_linear is None:\n",
    "            self._to_linear = x[0].shape[0]*x[0].shape[1]*x[0].shape[2]\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.convs(x)\n",
    "        x = x.view(-1, self._to_linear)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.softmax(x, dim=1)\n",
    "\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loading_model():\n",
    "\n",
    "    model_save_path = \"C:/Meh/jupyter nb/ML Minor Project/model save/CNNmodel_new.pth\"\n",
    "    #model_save_path = \"D:/CNNmodel_new.pth\"\n",
    "    mynet = torch.load(model_save_path)\n",
    "    return mynet\n",
    "\n",
    "MyNN = loading_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = {0:'A', 1:'B', 2:'C', 3:'D', 4:'E', 5:'F', 6:'G', 7:'H', 8:'I', 9:'J', 10:'K', 11:'L', 12:'M', 13:'N', 14:'O', 15:'P', 16:'Q', 17:'R', 18:'S', 19:'T', 20:'U', 21:'V', 22:'W', 23:'X', 24:'Y', 25:'Z', 26:'_', 27:'*'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n",
      "text: \n",
      "*\n",
      "text: \n",
      "M\n",
      "text: \n",
      "Y\n",
      "text: \n",
      "C\n",
      "text: \n",
      "M\n",
      "text: \n",
      "A\n",
      "text: \n",
      "M\n",
      "text: \n",
      "P\n",
      "text: \n",
      "A\n",
      "text: \n",
      "A\n",
      "text: \n",
      "_\n",
      "text: \n",
      "_\n",
      "text: \n",
      "_\n",
      "text: \n",
      "P\n",
      "text: \n",
      "A\n",
      "text: \n",
      "A\n",
      "text: \n",
      "A\n",
      "text: \n",
      "Y\n",
      "text: \n",
      "A\n",
      "text: \n",
      "T\n",
      "text: \n",
      "P\n",
      "text: \n",
      "A\n",
      "text: \n",
      "A\n",
      "text: \n",
      "L\n",
      "text: \n",
      "Y\n",
      "text: \n",
      "A\n",
      "text: \n",
      "N\n",
      "text: \n",
      "H\n",
      "text: \n",
      "A\n",
      "text: \n",
      "A\n",
      "text: \n",
      "A\n",
      "text: \n",
      "A\n",
      "text: \n",
      "C\n",
      "text: \n",
      "A\n",
      "text: \n",
      "C\n",
      "text: \n",
      "A\n",
      "text: \n",
      "C\n",
      "text: \n",
      "Y\n",
      "text: \n",
      "A\n",
      "text: \n",
      "A\n",
      "text: \n",
      "A\n",
      "text: \n",
      "S\n",
      "text: \n",
      "A\n",
      "text: A\n",
      "S\n",
      "text: A\n",
      "Y\n",
      "text: A\n",
      "S\n",
      "text: A\n",
      "E\n",
      "text: A\n",
      "S\n",
      "text: A\n",
      "Y\n",
      "text: A\n",
      "A\n",
      "text: A\n",
      "A\n",
      "text: A\n",
      "A\n",
      "text: A\n",
      "V\n",
      "text: A\n",
      "F\n",
      "text: A\n",
      "V\n",
      "text: A\n",
      "V\n",
      "text: A\n",
      "U\n",
      "text: A\n",
      "V\n",
      "text: A\n",
      "V\n",
      "text: A\n",
      "V\n",
      "text: A\n",
      "B\n",
      "text: A\n",
      "V\n",
      "text: A\n",
      "V\n",
      "text: A\n",
      "V\n",
      "text: A\n",
      "V\n",
      "text: A\n",
      "V\n",
      "text: A\n",
      "V\n",
      "text: AV\n",
      "V\n",
      "text: AV\n",
      "V\n",
      "text: AV\n",
      "V\n",
      "text: AV\n",
      "V\n",
      "text: AV\n",
      "V\n",
      "text: AV\n",
      "V\n",
      "text: AV\n",
      "V\n",
      "text: AV\n",
      "A\n",
      "text: AV\n",
      "B\n",
      "text: AV\n",
      "Z\n",
      "text: AV\n",
      "U\n",
      "text: AV\n",
      "_\n",
      "text: AV\n",
      "_\n",
      "text: AV\n",
      "J\n",
      "text: AV\n",
      "_\n",
      "text: AV\n",
      "Z\n",
      "text: AV\n",
      "Z\n",
      "text: AV\n",
      "Z\n",
      "text: AV\n",
      "Z\n",
      "text: AV\n",
      "Z\n",
      "text: AV\n",
      "F\n",
      "text: AV\n",
      "Z\n",
      "text: AV\n",
      "P\n",
      "text: AV\n",
      "Q\n",
      "text: AV\n",
      "*\n",
      "text: AV\n",
      "C\n",
      "text: AV\n",
      "C\n",
      "text: AV\n",
      "Z\n",
      "text: AV\n",
      "P\n",
      "text: AV\n",
      "P\n",
      "text: AV\n",
      "C\n",
      "text: AV\n",
      "*\n",
      "text: AV\n",
      "F\n",
      "text: AV\n",
      "P\n",
      "text: AV\n",
      "Z\n",
      "text: AV\n",
      "I\n",
      "text: AV\n",
      "Z\n",
      "text: AV\n",
      "C\n",
      "text: AV\n",
      "*\n",
      "text: AV\n",
      "F\n",
      "text: AV\n",
      "C\n",
      "text: AV\n",
      "_\n",
      "text: AV\n",
      "C\n",
      "text: AV\n",
      "*\n",
      "text: AV\n",
      "Z\n",
      "text: AV\n",
      "*\n",
      "text: AV\n",
      "_\n",
      "text: AV\n",
      "Z\n",
      "text: AV\n",
      "*\n",
      "text: AV\n",
      "P\n",
      "text: AV\n",
      "C\n",
      "text: AV\n",
      "Z\n",
      "text: AV\n",
      "*\n",
      "text: AV\n",
      "C\n",
      "text: AV\n",
      "Z\n",
      "text: AV\n",
      "*\n",
      "text: AV\n",
      "Z\n",
      "text: AV\n",
      "Z\n",
      "text: AV\n",
      "_\n",
      "text: AV\n"
     ]
    }
   ],
   "source": [
    "vid = cv2.VideoCapture(0)\n",
    "frame_list = []\n",
    "text = 'text: '\n",
    "guess = ''\n",
    "while True:\n",
    "    _, frame = vid.read()\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    \n",
    "    cv2.rectangle(frame, (320-1, 109), (620+1, 419), (255,0,0) ,2)\n",
    "    # seperating cropped image from the frame\n",
    "    \n",
    "    crop = frame[110:410, 320:620]\n",
    "\n",
    "    #cv2.imshow(\"Frame\", frame)\n",
    "    \n",
    "    #converting bgr colored image to grayscale\n",
    "    gray = cv2.cvtColor(crop, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    #blur = cv2.GaussianBlur(gray,(7,7),2)\n",
    "    blur = cv2.GaussianBlur(gray,(15,15),2)\n",
    "    \n",
    "    minValue = 240\n",
    "    #minValue = 130\n",
    "    adap_thres = cv2.adaptiveThreshold(blur,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,cv2.THRESH_BINARY_INV,11,2)\n",
    "    ret, test_image = cv2.threshold(adap_thres, minValue, 255, cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)\n",
    "    \n",
    "    \n",
    "    cv2.imshow(\"test\", test_image)\n",
    "    final_img = cv2.resize(test_image, (IMG_SIZE, IMG_SIZE))\n",
    "    interrupt = -1  \n",
    "    interrupt = cv2.waitKey(200)\n",
    "    if interrupt & 0xFF == 27: # esc key\n",
    "        break\n",
    "        \n",
    "    if interrupt:\n",
    "        net_out = MyNN(torch.Tensor(final_img).view(-1, 1, 150, 150))\n",
    "        arr = net_out[0].detach().numpy()\n",
    "        for i in range(len(arr)):\n",
    "            if int(arr[i])==1:\n",
    "                guess = LABELS[i]\n",
    "        cv2.putText(frame, str(guess), (450, 65), cv2.FONT_HERSHEY_PLAIN, 2, (0,0,255), 2)\n",
    "        print(str(guess))\n",
    "        \n",
    "    if interrupt & 0xFF == ord('p'):\n",
    "        text = text + str(guess)\n",
    "    \n",
    "    cv2.putText(frame, str(text), (10, 400), cv2.FONT_HERSHEY_PLAIN, 2, (0,255,), 2)\n",
    "    print(str(text))\n",
    "    \n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "        \n",
    "vid.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
